<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Heterogeneous Elastic Webserver on AWS by Rajul Bhatnagar</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Heterogeneous Elastic Webserver on AWS</h1>
        <h2>15618 Final Project</h2>
        <a href="https://github.com/rajulbhatnagar/15618project" class="button"><small>View project on</small> GitHub</a>
		
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
		<ul class="menu">
		  <li><a href="#">Final Report</a></li>
		  <li><a href="proposal.html">Project Proposal</a></li>
		  <li><a href="checkpoint.html">Project Checkpint</a></li>
		</ul>
		
		
          <h3>
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Summary</h3>

<p>I have implemented an Elastic Web Server on Amazon AWS which supports a heterogeneous server configuration and also allows heterogeneous scaling of different server
groups independently depending on the type of load. I then evaluated this against baseline round robin work distribution on four different traces having different combinations
of Compute and I/O characteristics and was able to demonstrate the reduced response latency of using this approach on a heterogeneous workload which is the characteristic of 
most real world web workloads.</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h3>

<p>Amazon offers an existing solution to create an Elastic Webserver on AWS using the Elastic Load Balancer and Auto Scale Group combination which fulfill some of the requirements of an Elastic Webserver like Availability and Elasticity however this approach has several limitations for real workloads which I look to address with my solution. 
I primarily addressed two limitations in ELB[1] + ASG[2] combination:-</p>

<ul>
<li>The ELB uses a naive Round Robin Load Balancing approach to balance incoming requests among various servers however this approach is 
inefficient when the workload associated with each request is imbalanced(heterogeneous) which is usually the case for real world loads. My system allows you to plug in 
you custom load balancing scheme depending on your requirements.</li>
<li>Auto Scale Group only allows Homogeneous scaling out of resources. We can achieve better performance for the same cost if we scale out by utilizing the knowledge of the type of workload which has 
increased(For e.g. launching a C series compute optimized instance may be more cost effective than launching M series instances).</li>
</ul>
<p>This allowed me to get lower latencies at the same AWS cost/hr of running the server.</p>

<h3>
<a id="the-approach" class="anchor" href="#the-approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach</h3>

<p>The basic architecture of my elastic webserver consists of a load balancer which runs on a EC2 instance in front of a cluster of different types of servers.<p>
<img src="images/architecture.jpg">
<p>Each Server Pool has a particular instance type which would be suitable for a 
particular type of job.The Load Balancer is built on top of the Undertow Web Framework[3] and implements the Undertow Proxy Client[4] interface.
The request arrives at the load balancer and then the server selection policy determines which cluster to use and which server to use in that cluster and sends the request to 
that server and the response back to the client.</p>

<h4>Configuring The System</h4>
<div >
<img style="width:50%;display:inline;float:left;margin-right:20px;" src="images/config1.jpg">
<span>
<span>For each Server Pool specify the following configuration:-</span>
<ul>
<li>
Instance Type
</li>
<li>
AMI ID(VM Image to load)
</li>
<li>
Minimum number of nodes(We boot up this many nodes in the beginning)
</li>
<li>
Maximum number of nodes(We donâ€™t scale beyond this many nodes)
</li>
<li>
scalingUpPolicy: When to scale out
</li>
<li>
scalingDownPolicy: When to scale in
</li>
</ul>
</span>
</div >
<div style="clear:both;padding-top:20px;">
<img style="width:50%;display:inline;float:left;margin-right:20px;" src="images/config2.jpg">
<span>
<span>For each Server Pool specify the scaling policy</span>
<ul>
<li>
Specify the Cloudwatch metric to use.
</li>
<li>
Specify the statistic to use for that metric(eg Average,Min,Max etc..)
</li>
<li>
Specify the time period after which to evaluate the policy
</li>
<li>
Specify the number of instances to launch or terminate
</li>
<li>
Specify the thresholds within which the policy should trigger
</li>
<li>
Specify the time to wait after the VM has been provisioned to give time for OS to boot
</li>
</ul>
</span>
</div>
<p style="clear:both;padding-top:20px">To evaluate the system I created traces containing four types of requests three of them were ported to java from the 15418 assignment 4[] and I created an
additional I/O intensive request.</p>
<h5>Computationally Expensive Requests</h5>
<ul>
<li><b>418wisdom:</b>This request invokes the "418 oracle", a highly sophisticated, but secret, 
algorithm that accepts as input an integer and uses it to generate a response that conveys great wisdom about how to succeed in 418. 
Given the sophistication of this algorithm, it is highly CPU intensive. It requires very little input data as so its footprint and 
bandwidth requirements are negligible. The algorithm does approximately the same amount of work for every invocation, so running times will be argument independent.</li>
<li><b>countprimes:</b>This request accepts an integer argument n and returns the number of prime numbers between 0 and n. 
The task has similar workload characteristics as 418wisdom. It is CPU intensive with little to no bandwidth or memory 
footprint requirements. However unlike 418wisdom, the runtime for countprimes requests is variable depending on the value of n.
Smaller values of n result in cheaper requests.</li>
</ul>
<h5>Latency Critical Requests</h5>
<ul>
<li><b>tellmenow:</b>This request is an automatic "office hours" service for Assignment 4. Students want help, and they want it now, so the 
there is a very strict response latency requirement for tellmenow requests. Tellmenow requests are very cheap, requiring only a few CPU ops to process.</li>
</ul>
<h5>I/O Intensive Requests</h5>
<ul>
<li><b>memorykiller:</b>This request takes a image or video file name as input. If the requested resource is found in the inmemory local memcached[5] then it is
loaded from memory otherwise the resource is read from disk.The request is served much faster if we get a cache hit. There is no CPU requirements apart from that
required by memcached service to fetch the resource. This request simulates the real world case of serving static resources.The data can be found <a href="http://15418.s3-website-us-east-1.amazonaws.com/memorykiller/">here(1.jpg-2000.jpg and 1.mpg-500.mpg)</a></li>
</ul>

<h3>
<a id="resources" class="anchor" href="#resources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results</h3>
<h4>System Configurations</h4>
<p>Given below are the three configurations that are compared. For all three configurations needed to satisfy the normalized
on demand running cost for Server + Load Balancer needed to be under 1.6$/hr on AWS. This is calculated as the average number of seconds run 
by all instances normalized to 1hr for each trace. For all the tests using my Load Balancer the Load Balancer was run on a C4.large instance.</p>
<ul>
<li><b>Homogeneous Server Pool with Round Robin Scheduling</b> of all requests. This is the baseline system behind the AWS Elastic Load Balancer.<br>
<table>
<thead>
<tr>
<th>Instance Type</th>
<th>Quantity</th>
<th>vCPU</th>
<th>ECU</th>
<th>Memory</th>
<th>AWS Hourly Cost($/hr)</th>
</tr>
</thead>
<tbody>
<tr>
<td>m4.large</td>
<td>12</td>
<td>2</td>
<td>6.5</td>
<td>8</td>
<td>0.12</td>
</tr>
</tbody>
</table>
<br>
</li>
<li><b>Heterogeneous Server Pool without Scaling</b> of all requests. This system is behind my Load Balancer.</li><br>
<table>
<thead>
<tr>
<th>Instance Type</th>
<th>Quantity</th>
<th>vCPU</th>
<th>ECU</th>
<th>Memory</th>
<th>AWS Hourly Cost($/hr)</th>
</tr>
</thead>
<tbody>
<tr>
<td>c4.large</td>
<td>12</td>
<td>2</td>
<td>8</td>
<td>3.75</td>
<td>0.105</td>
</tr>
<tr>
<td>r4.large</td>
<td>1</td>
<td>2</td>
<td>6.5</td>
<td>15</td>
<td>0.166</td>
</tr>
<tr>
<td>t2.micro</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0.013</td>
</tr>
</tbody>
</table>
<br>
<li><b>Heterogeneous Server Pool with Scaling</b> of all requests. This is the baseline system behind the Elastic Load Balancer.</li><br>
<table>
<thead>
<tr>
<th>Instance Type</th>
<th>Min</th>
<th>Max</th>
<th>vCPU</th>
<th>ECU</th>
<th>Memory</th>
<th>AWS Hourly Cost($/hr)</th>
</tr>
</thead>
<tbody>
<tr>
<td>c4.large</td>
<td>2</td>
<td>16</td>
<td>2</td>
<td>8</td>
<td>3.75</td>
<td>0.105</td>
</tr>
<tr>
<td>r3.large</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>6.5</td>
<td>15</td>
<td>0.166</td>
</tr>
<tr>
<td>t2.micro</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0.013</td>
</tr>
</tbody>
</table>
<h6>*Note</h6>
<ul>
<li>There was a requirement for all the configurations to be under 1.6$/hr normalize cost</li>
<li>Results for Round Robin with Scaling are not included because even with 1.5x number of instances available
and more cost than the limit the results were much worse than heterogeneous system without scaling</li>
</ul>
<h6>Scaling Policy</h6>
<p><b>c4.large:</b>For this instance pool we have two conditions to scale up. If Average CPUUtilisation for 20 second period is between 30-60 add four instances
and if between 60-100 add another four instances.We can merge these two into one and it produces the same result. For scale down policy if 
the average CPU utilisation is below 15 for 20 second period we remove 2 instances.</p>
<p><b>r3.large:</b>I tried horizontal scaling with a distributed memcached configuration but the overhead of transfering data over the network to the memcached server was too much 
and degraded performance thus I went with vertical scaling by using a memory optimised r3 instance.</p>
<p><b>t2.micro:</b>For tellmeknow requests a single of this type of instance is sufficient.</p>
</ul>
<h4>Trace Characteristics</h4>
<p>To evaluate the system I created four different traces having different characteristics:-</p>
<ul>
<li>
<b>Trace 1:</b> is a mix of I/O and compute.<br>
<table>
<thead>
<tr><th style="width:30%;">Request Type</th><th>Quantity</th></tr>
</thead>
<tbody>
<tr><td>memorykiller</td><td>2600</td></tr>
<tr><td>418wisdom</td><td>1000</td></tr>
<tr><td>computeprimes</td><td>800</td></tr>
</tbody>
</table>
<br>
<p>Both 418wisdom and Computeprimes arrive based on a normal distribution of the number of requests for 12 minute period. 
The memory killer requests arrive in two batches of normal distributions over a period of 6 minutes each. A lot of these requests
are repeated more than once across the batches so there is opportunity for caching. Also 5/6 of the total requests are image requests(2-3 MB) read
from disk in worst case whereas the other remaining 1/6th are video file requests of 20MB.</p>
</li>
<li>
<b>Trace 2:</b> is a mix of I/O and compute along with requests having strict latency requirement.<br>
<table>
<thead>
<tr><th style="width:30%;">Request Type</th><th>Quantity</th></tr>
</thead>
<tbody>
<tr><td>memorykiller</td><td>3600</td></tr>
<tr><td>tellmenow</td><td>2000</td></tr>
<tr><td>computeprimes</td><td>1000</td></tr>
</tbody>
</table>
<br>
<p>Both tellmenow and Computeprimes arrive based on a normal distribution of the number of requests for 13 minute period. 
The memory killer requests arrive in two batches of normal distributions over a period of 6.5 minutes each. A lot of these requests
are repeated more than once across the batches so there is opportunity for caching. Also 5/6 of the total requests are image requests(2-3 MB) read
from disk in worst case whereas the other remaining 1/6th are video file requests of 20MB. The idea of this trace is to measure the 
systems ability to deal with the latency critical requests when we have I/O and compute bound jobs that could potentially affect response times of this
latency critical job.</p>
</li>
<li>
<b>Trace 3:</b> is a mix of compute along with requests having strict latency requirement.<br>
<table>
<thead>
<tr><th style="width:30%;">Request Type</th><th>Quantity</th></tr>
</thead>
<tbody>
<tr><td>tellmenow</td><td>300</td></tr>
<tr><td>418wisdom</td><td>1900</td></tr>
</tbody>
</table>
<br>
<p>Both tellmenow and 418 arrive based on a normal distribution of the number of requests for 10 minute period. 
The idea of this trace is to measure the systems ability to deal with the latency critical requests when we have compute bound jobs that could potentially affect 
response times of this latency critical job.</p>
</li>
<li>
<b>Trace 4:</b> is a mix of I/O and compute.<br>
<table>
<thead>
<tr><th style="width:30%;">Request Type</th><th>Quantity</th></tr>
</thead>
<tbody>
<tr><td>tellmenow</td><td>~1000</td></tr>
<tr><td>418wisdom</td><td>~1000</td></tr>
<tr><td>computeprimes</td><td>~1000</td></tr>
</tbody>
</table>
<br>
<p>This trace was created by creating a normal distribution of 3000 arrival time points over a 11 minute period 
and then for each time point one of the three type of request was sent at that moment. This simulates a random heterogeneous workload
with normally distributed arrival times and we use this to measure the systems ability to handle latency critical as well as computationally
heavy workload.
</p>
</li>
</ul>

<h4>Trace 1 Results</h4>
<div>
<p>We see that in the homogeneous configuration we can see that when the arrival rate increases we see a very
big spike in latency for all three types of requests.We see this spike for all three types of requests. 
The server pool is just not able to keep up with the requests. We can see
in the heterogeneous configuration without scaling that there is a small spike for both the compute
tasks(418wisdom and countprimes) however it is substantially lesser than the baseline configuration.
The reason for this is because the I/O bound requests are not served by the compute pool
of servers so those dont cause compute requests to backup. Also as the I/O requests are 
served by a instance with memory big enough to cache all requests all repeating requests are 
served from memcached. With the scaling configuration we see virtually no spike in latency for 
all three type of requests. We do observe mini spikes in the begenning and in the middle
and this coincides with the scaling activity as scaling is not free and instances take time to spin up.</p>

<img src="images/results/trace1/input1.jpg">
<img src="images/results/trace1/response1.jpg">
<img src="images/results/trace1/input2.jpg">
<img src="images/results/trace1/response2.jpg">
<img src="images/results/trace1/input3.jpg">
<img src="images/results/trace1/response3.jpg">

</div>
<h4>Trace 2</h4>
<div>
<p>We see that in the homogeneous configuration the tellmeknow requests which can be served under 1ms are being queued up.
We can see in the heterogeneous configuration with and without scaling these requests are always served instantly
 and this is because the compute server pool does not handle these requests. Also with scaling we see that there is
 no spike in latency during peak load however there are a few spikes in between when the scaling activity is going
 on and the worker is not yet online.</p>

<img src="images/results/trace2/input1.jpg">
<img src="images/results/trace2/response1.jpg">
<img src="images/results/trace2/input2.jpg">
<img src="images/results/trace2/response2.jpg">
<img src="images/results/trace2/input3.jpg">
<img src="images/results/trace2/response3.jpg">
</div>
<h4>Trace 3</h4>
<div>
<p>Here we again see that the latency critical requests are getting queued up in the homogeneous
configuration due to the workers being overwhelmed by compute.Even the very few tellmeknow requests cant
be processed and get backedup. Also we can see scaling helping with the spike in request latency when the 
arrival rate becomes very high.Also the dedicated node to handle latency critical but no compute requests 
is critical in ensuring that those requests never get backed up.</p>
<img src="images/results/trace3/input1.jpg">
<img src="images/results/trace3/response1.jpg">
<img src="images/results/trace3/input2.jpg">
<img src="images/results/trace3/response2.jpg">
</div>
<h4>Trace 4</h4>
<div>
<p>In this trace also as the three requests arrive at random with the overall arrival rate following a normal distribution
we see that there is a lot of spikes in latencies throughout the run for the baseline
homogenous configuration but with a heterogeneous configuration the latency critical requests
no longer get backed up. Also with elastic scaling we get rid of the spikes due to arrival 
rate increase but there will be small spikes during scaling activity.</p>
<img src="images/results/trace4/input1.jpg">
<img src="images/results/trace4/response1.jpg">
<img src="images/results/trace4/input2.jpg">
<img src="images/results/trace4/response2.jpg">
<img src="images/results/trace4/input3.jpg">
<img src="images/results/trace4/response3.jpg">
</div>
<h4>Result Summary</h4>
<img src="images/summary.jpg">
<h3>Takeaways</h3>
<p>The above traces help in demonstrating a lot of interesting insights. Some of the key takeaways are:-</p>
<ul>
<li>
Most web workloads are heterogeneous in nature and thus Round Robin scheduling in a homogeneous server pool can cause significant increase in latency
</li>
<li>
With a heterogeneous system we can use the most optimum resource type to serve the request to achieve significantly better performance at the same cost($/hr in this case)
</li>
<li>
Scaling Out is not free as it takes time to detect increase in workload and to spin up servers
</li>
</ul>
<h3>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h3>
<ol>
<li><a href="https://aws.amazon.com/elasticloadbalancing/">Elastic Load Balancer</a></li>
<li><a href="https://aws.amazon.com/autoscaling/">Auto Scale Group</a></li>
<li><a href="http://undertow.io/">Undertow</a></li>
<li><a href="http://undertow.io/javadoc/1.2.x/io/undertow/server/handlers/proxy/ProxyClient.html">Undertow Proxy Client</a></li>
<li><a href="https://memcached.org/">Memcached</a></li>
</ol>
        </section>

        <aside id="sidebar">
          <a href="https://github.com/rajulbhatnagar/15618project/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/rajulbhatnagar/15618project/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/rajulbhatnagar/15618project"></a> is maintained by <a href="https://github.com/rajulbhatnagar">rajulbhatnagar</a>.</p>

         </aside>
      </div>
    </div>

  
  </body>
</html>
