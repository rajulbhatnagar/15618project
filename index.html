<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Architects+Daughter' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <title>Heterogeneous Elastic Webserver on AWS by Rajul Bhatnagar</title>
  </head>

  <body>
    <header>
      <div class="inner">
        <h1>Heterogeneous Elastic Webserver on AWS</h1>
        <h2>15618 Final Project</h2>
        <a href="https://github.com/rajulbhatnagar/15618project" class="button"><small>View project on</small> GitHub</a>
		
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
		<ul class="menu">
		  <li><a href="#">Final Report</a></li>
		  <li><a href="proposal.html">Project Proposal</a></li>
		  <li><a href="checkpoint.html">Project Checkpint</a></li>
		</ul>
		
		
          <h3>
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Summary</h3>

<p>I have implemented an Elastic Web Server on Amazon AWS which supports a heterogeneous server configuration and also allows heterogeneous scaling of differant server
groups independently depending on the type of load. I then evaluated this against baseline round robin work distribution on four differant traces having differant combinations
of Compute and I/O characteristics and was able to demonstrate the reduced response latency of using this approach on a heterogeneous workload which is the characteristic of 
most real world web workloads.</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h3>

<p>Amazon offers an existing solution to create an Elastic Webserver on AWS using the Elastic Load Balancer and Auto Scale Group combination which fulfill some of the requirements of an Elastic Webserver like Availability and Elasticity however this approach has several limitations for real workloads which I look to address with my solution. 
I primarily addressed two limitations in ELB + ASG combination:-</p>

<ul>
<li>The ELB uses a naive Round Robin Load Balancing approach to balance incoming requests among various servers however this approach is 
inefficient when the workload associated with each request is imbalanced(heterogeneous) which is usually the case for real world loads. My system allows you to plug in 
you custom load balancing scheme depending on your requirements.</li>
<li>Auto Scale Group only allows Homogeneous scaling out of resources. We can achieve better performance for the same cost if we scale out []ly utilizing the knowledge of the type of workload which has 
increased(For e.g. launching a C series compute optimized instance may be more cost effective than launching M series instances).</li>
</ul>
<p>This allowed me to get lower latencies at the same AWS cost/hr of running the server.</p>

<h3>
<a id="the-approach" class="anchor" href="#the-approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Approach</h3>

<p>The basic architecture of my elastic webserver consists of a load balancer which runs on a EC2 instance in front of a cluster of differant types of servers.<p>
<p>TODO Architecture Image</p>
<p>You specify your backend instances connected to the load balancer through a json configuration file <a href="images/config.json">Example file till I create images explaining the schema</a>.
The Load Balancer is built on top of the Undertow Web Framework[] and implements the Undertow Proxy Client[] interface.</p>
<p>The request arrives at the load balancer and then the server selection policy determines which cluster to use and which server to use in that cluster and sends the request to 
that server and the response back to the client. To evaluate the system I created traces containing four types of requests three of them were ported to java from the 15418 assignment 4[] and I created an
additional I/O intensive request.</p>
<h5>Computationally Expensive Requests</h5>
<ul>
<li><b>418wisdom:</b>This request invokes the "418 oracle", a highly sophisticated, but secret, 
algorithm that accepts as input an integer and uses it to generate a response that conveys great wisdom about how to succeed in 418. 
Given the sophistication of this algorithm, it is highly CPU intensive. It requires very little input data as so its footprint and 
bandwidth requirements are negligible. The algorithm does approximately the same amount of work for every invocation, so running times will be argument independent.</li>
<li><b>countprimes:</b>This request accepts an integer argument n and returns the number of prime numbers between 0 and n. 
The task has similar workload characteristics as 418wisdom. It is CPU intensive with little to no bandwidth or memory 
footprint requirements. However unlike 418wisdom, the runtime for countprimes requests is variable depending on the value of n.
Smaller values of n result in cheaper requests.</li>
</ul>
<h5>Latency Critical Requests</h5>
<ul>
<li><b>tellmenow:</b>This request is an automatic "office hours" service for Assignment 4. Students want help, and they want it now, so the 
there is a very strict response latency requirement for tellmenow requests. Tellmenow requests are very cheap, requiring only a few CPU ops to process.</li>
</ul>
<h5>I/O Intensive Requests</h5>
<ul>
<li><b>memorykiller:</b>This request takes a image or video file name as input. If the requested resource is found in the inmemory local memcached[] then it is
loaded from memory otherwise the resource is read from disk.The request is served much faster if we get a cache hit. There is no CPU requirements apart from that
required by memcached service to fetch the resource. This request simulates the real world case of serving static resources.</li>
</ul>

<h3>
<a id="resources" class="anchor" href="#resources" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Results</h3>
<h4>System Configurations</h4>
<p>Given below are the three configurations that are compaired. For all three configurations needed to satisfy the normalized
on demand running cost for Server + Load Balancer needed to be under 1.6$/hr on AWS. This is calculated as the average number of seconds run 
by all instances normalized to 1hr for each trace. For all the tests using my Load Balancer the Load Balancer was run on a C4.large instance.</p>
<ul>
<li><b>Homogeneous Server Pool with Round Robin Scheduling</b> of all requests. This is the baseline system behind the AWS Elastic Load Balancer.<br>
<table>
<thead>
<tr>
<th>Instance Type</th>
<th>Quantity</th>
<th>vCPU</th>
<th>ECU</th>
<th>Memory</th>
<th>AWS Hourly Cost($/hr)</th>
</tr>
</thead>
<tbody>
<tr>
<td>m4.large</td>
<td>12</td>
<td>2</td>
<td>6.5</td>
<td>8</td>
<td>0.12</td>
</tr>
</tbody>
</table>
<br>
</li>
<li><b>Heterogeneous Server Pool without Scaling</b> of all requests. This system is behind my Load Balancer.</li><br>
<table>
<thead>
<tr>
<th>Instance Type</th>
<th>Quantity</th>
<th>vCPU</th>
<th>ECU</th>
<th>Memory</th>
<th>AWS Hourly Cost($/hr)</th>
</tr>
</thead>
<tbody>
<tr>
<td>c4.large</td>
<td>12</td>
<td>2</td>
<td>8</td>
<td>3.75</td>
<td>0.105</td>
</tr>
<tr>
<td>r4.large</td>
<td>1</td>
<td>2</td>
<td>6.5</td>
<td>15</td>
<td>0.166</td>
</tr>
<tr>
<td>t2.micro</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0.013</td>
</tr>
</tbody>
</table>
<br>
<li><b>Heterogeneous Server Pool with Scaling</b> of all requests. This is the baseline system behind the Elastic Load Balancer.</li><br>
<table>
<thead>
<tr>
<th>Instance Type</th>
<th>Min</th>
<th>Max</th>
<th>vCPU</th>
<th>ECU</th>
<th>Memory</th>
<th>AWS Hourly Cost($/hr)</th>
</tr>
</thead>
<tbody>
<tr>
<td>c4.large</td>
<td>2</td>
<td>16</td>
<td>2</td>
<td>8</td>
<td>3.75</td>
<td>0.105</td>
</tr>
<tr>
<td>r3.large</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>6.5</td>
<td>15</td>
<td>0.166</td>
</tr>
<tr>
<td>t2.micro</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0.013</td>
</tr>
</tbody>
</table>
<h6>Scaling Policy</h6>
<p><b>c4.large:</b>For this instance pool we have two conditions to scale up. If Average CPUUtilisation for 20 second period is between 30-60 add four instances
and if between 60-100 add another four instances.We can merge these two into one and it produces the same result. For scale down policy if 
the average CPU utilisation is below 15 for 20 second period we remove 2 instances.</p>
<p><b>r3.large:</b>I tried horizontal scaling with a distributed memcached configuration but the overhead of transfering data over the network to the memcached server was too much 
and degraded performance thus I went with vertical scaling by using a memory optimised r3 instance.</p>
<p><b>t2.micro:</b>For tellmeknow requests a single of this type of instance is sufficient.</p>
</ul>
<h4>Trace Characteristics</h4>
<p>To evaluate the system I created four differant traces having differant characteristics:-</p>
<ul>
<li>
<b>Trace 1:</b> is a mix of I/O and compute.<br>
<table>
<thead>
<tr><th style="width:30%;">Request Type</th><th>Quantity</th></tr>
</thead>
<tbody>
<tr><td>memorykiller</td><td>2600</td></tr>
<tr><td>418wisdom</td><td>1000</td></tr>
<tr><td>computeprimes</td><td>800</td></tr>
</tbody>
</table>
<br>
<p>Both 418wisdom and Computeprimes arrive based on a normal distribution of the number of requests for 12 minute period. 
The memory killer requests arrive in two batches of normal distributions over a period of 6 minutes each. A lot of these requests
are repeated more than once accross the batches so there is oppertunity for caching. Also 5/6 of the total requests are image reqeusts(2-3 MB) read
from disk in worst case whereas the other remaining 1/6th are video file requests of 20MB.</p>
</li>
<li>
<b>Trace 2:</b> is a mix of I/O and compute along with requests having strict latency requirement.<br>
<table>
<thead>
<tr><th style="width:30%;">Request Type</th><th>Quantity</th></tr>
</thead>
<tbody>
<tr><td>memorykiller</td><td>3600</td></tr>
<tr><td>tellmenow</td><td>2000</td></tr>
<tr><td>computeprimes</td><td>1000</td></tr>
</tbody>
</table>
<br>
<p>Both tellmenow and Computeprimes arrive based on a normal distribution of the number of requests for 13 minute period. 
The memory killer requests arrive in two batches of normal distributions over a period of 6.5 minutes each. A lot of these requests
are repeated more than once accross the batches so there is oppertunity for caching. Also 5/6 of the total requests are image reqeusts(2-3 MB) read
from disk in worst case whereas the other remaining 1/6th are video file requests of 20MB. The idea of this trace is to measure the 
systems ability to deal with the latency critical requests when we have I/O and compute bound jobs that could potentially affect response times of this
latency critical job.</p>
</li>
<li>
<b>Trace 3:</b> is a mix of along with requests having strict latency requirement.<br>
<table>
<thead>
<tr><th style="width:30%;">Request Type</th><th>Quantity</th></tr>
</thead>
<tbody>
<tr><td>tellmenow</td><td>300</td></tr>
<tr><td>418wisdom</td><td>1900</td></tr>
</tbody>
</table>
<br>
<p>Both tellmenow and 418 arrive based on a normal distribution of the number of requests for 10 minute period. 
The idea of this trace is to measure the systems ability to deal with the latency critical requests when we have compute bound jobs that could potentially affect 
response times of this latency critical job.</p>
</li>
<li>
<b>Trace 4:</b> is a mix of I/O and compute.<br>
<table>
<thead>
<tr><th style="width:30%;">Request Type</th><th>Quantity</th></tr>
</thead>
<tbody>
<tr><td>tellmenow</td><td>~1000</td></tr>
<tr><td>418wisdom</td><td>~1000</td></tr>
<tr><td>computeprimes</td><td>~1000</td></tr>
</tbody>
</table>
<br>
<p>This trace was created by creating a normal distribution of 3000 arrival time points over a 11 minute period 
and then for each time point one of the three type of request was sent at that moment. This simulates a random heterogeneous workload
with normally distributed arrival times and we use this to measure the systems ability to handle latency critical as well as computationally
heavy workload.
</p>
</li>
</ul>

<h4>Trace 1 Results</h4>
<div>
<img src="images/results/trace1/input1.jpg">
<img src="images/results/trace1/response1.jpg">
<p>Bla</p>
<img src="images/results/trace1/input2.jpg">
<img src="images/results/trace1/response2.jpg">
<p>Bla</p>
<img src="images/results/trace1/input3.jpg">
<img src="images/results/trace1/response3.jpg">
<p>Bla</p>
</div>
<h4>Trace 2</h4>
<div>
<img src="images/results/trace2/input1.jpg">
<img src="images/results/trace2/response1.jpg">
<img src="images/results/trace2/input2.jpg">
<img src="images/results/trace2/response2.jpg">
<img src="images/results/trace2/input3.jpg">
<img src="images/results/trace2/response3.jpg">
</div>
<h4>Trace 3</h4>
<div>
<img src="images/results/trace3/input1.jpg">
<img src="images/results/trace3/response1.jpg">
<img src="images/results/trace3/input2.jpg">
<img src="images/results/trace3/response2.jpg">
</div>
<h4>Trace 4</h4>
<div>
<img src="images/results/trace4/input1.jpg">
<img src="images/results/trace4/response1.jpg">
<img src="images/results/trace4/input2.jpg">
<img src="images/results/trace4/response2.jpg">
<img src="images/results/trace4/input3.jpg">
<img src="images/results/trace4/response3.jpg">
</div>

<h3>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>References</h3>

<p>Coming Soon</p>
        </section>

        <aside id="sidebar">
          <a href="https://github.com/rajulbhatnagar/15618project/zipball/master" class="button">
            <small>Download</small>
            .zip file
          </a>
          <a href="https://github.com/rajulbhatnagar/15618project/tarball/master" class="button">
            <small>Download</small>
            .tar.gz file
          </a>

          <p class="repo-owner"><a href="https://github.com/rajulbhatnagar/15618project"></a> is maintained by <a href="https://github.com/rajulbhatnagar">rajulbhatnagar</a>.</p>

         </aside>
      </div>
    </div>

  
  </body>
</html>
